/home/iscilab/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:658: Checkpoint directory /data2/douyoungk/checkpoints exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
  | Name            | Type                     | Params | Mode
---------------------------------------------------------------------
0 | context_encoder | ContextEncoder           | 192 K  | train
1 | traj_encoder    | FourHotTrajectoryEncoder | 9.6 M  | train
2 | fusion_encoder  | FusionEncoder            | 9.5 M  | train
3 | decoder         | AISDecoder               | 17.2 M | train
---------------------------------------------------------------------
36.4 M    Trainable params
0         Non-trainable params
36.4 M    Total params
145.734   Total estimated model params size (MB)
145       Modules in train mode
0         Modules in eval mode
/home/iscilab/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/iscilab/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_pred_error', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/iscilab/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.






Epoch 8: 100%|█████████████████████| 6/6 [00:01<00:00,  4.93it/s, v_num=81yl, train_loss=nan.0, val_loss=nan.0, val_pred_error=172.0]




















Epoch 41: 100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  5.00it/s, v_num=81yl, train_loss=nan.0, val_loss=nan.0, val_pred_error=172.0]





Epoch 49:  83%|██████████████████████████████████████████████████████████████████████              | 5/6 [00:01<00:00,  4.38it/s, v_num=81yl, train_loss=nan.0, val_loss=nan.0, val_pred_error=172.0]
`Trainer.fit` stopped: `max_epochs=50` reached.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/home/iscilab/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:216: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
Epoch 49: 100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  4.66it/s, v_num=81yl, train_loss=nan.0, val_loss=nan.0, val_pred_error=172.0]
Testing DataLoader 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 39.37it/s]
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
     test_pred_error        188.10708618164062
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────